# -*- coding: utf-8 -*-
"""ModelDevelopment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BDfDawdqLRJwVh0dRtPhBDxFlbPZtVee

### TABLE OF CONTENTS

- I.Data Import
- II. Data Exploration
- III. Train-Dev Split
- IV. Tokenization
- V. Embeddings
- VI. Performance Evaluation
- VII. DNN Model
- VIII. CNN Model
- IX. CNN + LSTM Model
- X. RNN BLSTM Models

### I. Data Import

Data Source:

Toxic Comment Classification Challenge

https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

TRAIN_DATA_FILE= '/content/drive/MyDrive/ContentModerationProject/Final/train.csv'

max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)
maxlen = 50 # max number of words in a comment to use
embed_size = 100

train = pd.read_csv(TRAIN_DATA_FILE)

train.shape

train.head()

"""### II. Data Exploration"""

# Check the dimensions
print(f"Number of rows: {train.shape[0]}\n"
      f"Number of columns: {train.shape[1]}")

# Check for missing values
print(f"Number of missing values: {train.isna().sum().sum()}")

# Inspect the classes
cols = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]
classes = [train[col].value_counts(normalize = True) for col in cols]
pd.DataFrame(classes, index = cols)

# Set up the figure with subplots
plt.rcParams.update({"font.family": "serif"})
fig, axs = plt.subplots(nrows = 2, ncols = 3, figsize = (12, 8.5))

# Visualize the class distribution for each column
for i, col in enumerate(cols):
    counts = train[col].value_counts()
    axs[i // 3, i % 3].bar(counts.index, counts.values, color = ["#C5E1E7", "#7DC6D4"])
    axs[i // 3, i % 3].set_title(col)
    axs[i // 3, i % 3].set_xlabel("Value")
    axs[i // 3, i % 3].set_ylabel("Count")

# Display the plot
fig.suptitle("Class Distribution", size = 20) # Assign main title
fig.tight_layout()

"""### III. Train-Dev Split"""

from sklearn.model_selection import train_test_split

X_train, X_dev, y_train, y_dev = train_test_split(train["comment_text"], train[cols], 
                                                  test_size = 0.2, random_state = 10)

X_train = np.array(X_train).reshape(X_train.shape[0], 1)
X_dev = np.array(X_dev).reshape(X_dev.shape[0], 1)

print(X_train.shape, X_dev.shape, y_train.shape, y_dev.shape)

"""### IV. Tokenization"""

import re
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences

lemmatizer = WordNetLemmatizer()
sw = set(stopwords.words('english'))

# Need to convert treebank_tag to wordnet tag before lemmatization
# reference code: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python

def get_wordnet_pos(treebank_tag):

    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return None

def postag_lemmatize(t):
    l = []
    tagged_t = nltk.pos_tag(t)
    for word, tag in tagged_t:
        wntag = get_wordnet_pos(tag)
        if wntag is None:
            l.append(lemmatizer.lemmatize(word))
        else:
            l.append(lemmatizer.lemmatize(word, pos=wntag))
    return l

def text_clean(t):
    t = re.sub(r'\n', ' ', t)
    t = re.sub(r'\t', ' ', t)
    t = t.lower()
    t = re.sub('[^a-z]', ' ', t)
    t = t.split()
    t = postag_lemmatize(t)
    t = [w for w in t if w not in sw]
    return t

comment_list_train = [str(item[0]) for item in X_train.tolist()]
comment_list_dev = [str(item[0]) for item in X_dev.tolist()]

tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(comment_list_train)

tokens_train = tokenizer.texts_to_sequences(comment_list_train)
tokens_dev = tokenizer.texts_to_sequences(comment_list_dev)

tokens_train = pad_sequences(tokens_train, maxlen=maxlen)
tokens_dev = pad_sequences(tokens_dev, maxlen=maxlen)

word_index = tokenizer.word_index

print(tokens_train.shape, tokens_dev.shape)

"""### V. Embeddings"""

!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip
!unzip /content/glove.6B.zip

EMBEDDING_FILE= 'glove.6B.100d.txt'

def build_word_dict(d, file):
    with open(file, 'r') as f:
        for line in f.readlines(): # read the file line by line
            line = line.split(' ') # split each line by space

            try:
                d[line[0]] = np.array(line[1:], dtype = 'float32') 
                # line[0] is the word, which is the key of the dictionary
                # line[1:] are embeddings, which is the value of the dictionary
            except: 
                continue

embeddings_dict = dict()
build_word_dict(embeddings_dict, EMBEDDING_FILE)

#Normalization
embed_vals = np.stack(embeddings_dict.values())
embed_mean, embed_std = embed_vals.mean(), embed_vals.std()
print(embed_mean, embed_std)

word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))
embedding_matrix = np.random.normal(embed_mean, embed_std, (nb_words, embed_size))
for word, i in word_index.items():
    if i >= max_features: continue
    embedding_vector = embeddings_dict.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector

"""### VI. Performance Evaluation"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

def float_array_to_binary(float_array, threshold):
    binary_array = np.where(np.array(float_array) >=  threshold, 1, 0)
    return binary_array

def chooseThreshold(subtype, true, pred, plot = True):
    max_f1 = 0
    argmax_f1 = 0
    accuracy_list = []
    recall_list = []
    precision_list = []
    f1_list = []
    thres_list = np.arange(0.1, 1.0, 0.1)

    for i in thres_list:
        i = round(i, 2)
        pred_i = float_array_to_binary(pred, i)
        accuracy = accuracy_score(true, pred_i)
        accuracy_list.append(accuracy)
        recall = recall_score(true, pred_i)
        recall_list.append(recall)
        precision = precision_score(true, pred_i)
        precision_list.append(precision)
        f1 = f1_score(true, pred_i)
        f1_list.append(f1)
        if f1 > max_f1:
            max_f1 = f1
            argmax_f1 = i
    
    if plot:
        plt.plot(thres_list, accuracy_list, label ='accuracy')
        plt.plot(thres_list, recall_list, '-.', label ='recall')
        plt.plot(thres_list, precision_list, '-', label ='precision')
        plt.plot(thres_list, f1_list, ':', label ='f1')
        plt.plot(argmax_f1, max_f1,'bo-')
        label = "(" + str(argmax_f1) + ", " + str(round(max_f1,2)) + ")"
        plt.annotate(label, (argmax_f1, max_f1), textcoords="offset points",
                        xytext=(0,10), ha='center') 

        plt.xlabel("threshold")
        plt.ylabel("performance")
        plt.legend()
        plt.title(subtype + ' Subtype : Performance At Different Threshold')
        plt.show()
    return argmax_f1, max_f1

def metrics(pred, true):
    conf_matrix = confusion_matrix(true, pred)
    tn, fp, fn, tp = conf_matrix.ravel()
    accuracy = accuracy_score(true, pred)
    precision = precision_score(true, pred)
    recall = recall_score(true, pred)
    f1 = f1_score(true, pred)
    return round(accuracy,4), round(precision,4), round(recall,4), round(f1,4)

def Model_OverallPerformance(y_pred, y_true, adjustThreshold = True, Thresholdplot = False):
    y_true_toxic = [row[0] for row in y_true]
    y_true_severe_toxic = [row[1] for row in y_true]
    y_true_obscene  = [row[2] for row in y_true]
    y_true_threat = [row[3] for row in y_true]
    y_true_insult = [row[4] for row in y_true]
    y_true_identity_threat = [row[5] for row in y_true]

    y_pred_toxic = [row[0] for row in y_pred]
    y_pred_severe_toxic = [row[1] for row in y_pred]
    y_pred_obscene  = [row[2] for row in y_pred]
    y_pred_threat = [row[3] for row in y_pred]
    y_pred_insult = [row[4] for row in y_pred]
    y_pred_identity_threat = [row[5] for row in y_pred]

    if adjustThreshold:
        thresh_toxic, f1_toxic = chooseThreshold('Toxic', y_true_toxic, y_pred_toxic, Thresholdplot)
        thresh_severe_toxic, f1_severe_toxic = chooseThreshold('Severe Toxic', y_true_severe_toxic, y_pred_severe_toxic, Thresholdplot)
        thresh_obscene, f1_obscene = chooseThreshold('Obsence', y_true_obscene, y_pred_obscene, Thresholdplot)
        thresh_threat, f1_threat = chooseThreshold('Threat', y_true_threat, y_pred_threat, Thresholdplot)
        thresh_insult, f1_insult = chooseThreshold('Insult', y_true_insult, y_pred_insult, Thresholdplot)
        thresh_identity_threat, f1_identity_threat = chooseThreshold('Identity Threat', y_true_identity_threat, y_pred_identity_threat, Thresholdplot)

        y_pred_toxic = float_array_to_binary(y_pred_toxic, thresh_toxic)
        y_pred_severe_toxic = float_array_to_binary(y_pred_severe_toxic, thresh_severe_toxic)
        y_pred_obscene = float_array_to_binary(y_pred_obscene, thresh_obscene)
        y_pred_threat = float_array_to_binary(y_pred_threat, thresh_threat)
        y_pred_insult = float_array_to_binary(y_pred_insult, thresh_insult)
        y_pred_identity_threat = float_array_to_binary(y_pred_identity_threat, thresh_identity_threat)

    accuracy_toxic, precision_toxic, recall_toxic, f1_toxic = metrics(y_pred_toxic, y_true_toxic)
    accuracy_severe_toxic, precision_severe_toxic, recall_severe_toxic, f1_severe_toxic = metrics(y_pred_severe_toxic, y_true_severe_toxic)
    accuracy_obscene, precision_obscene, recall_obscene, f1_obscene = metrics(y_pred_obscene, y_true_obscene)
    accuracy_threat, precision_threat, recall_threat, f1_threat = metrics(y_pred_threat, y_true_threat)
    accuracy_insult, precision_insult, recall_insult, f1_insult = metrics(y_pred_insult, y_true_insult)
    accuracy_identity_threat, precision_identity_threat, recall_identity_threat, f1_identity_threat = metrics(y_pred_identity_threat, y_true_identity_threat)


    accuracy_list = [accuracy_toxic, accuracy_severe_toxic, accuracy_obscene, accuracy_threat,accuracy_insult, accuracy_identity_threat]
    precision_list = [precision_toxic, precision_severe_toxic, precision_obscene, precision_threat,precision_insult, precision_identity_threat]
    recall_list = [ recall_toxic,  recall_severe_toxic,  recall_obscene,  recall_threat, recall_insult,  recall_identity_threat]
    f1_list = [f1_toxic, f1_severe_toxic, f1_obscene, f1_threat,f1_insult, f1_identity_threat]

    subtypes = ["Toxic", "Severe_Toxic", "Obscene", "Threat", "Insult", "Identity_Hate"]
    metric = ['Accuracy', 'Precision', 'Recall', 'F1']

    metrics_df = pd.DataFrame(
    data = [accuracy_list, precision_list, recall_list, f1_list],
    columns = subtypes,
    index = metric
    )
    print('\n')

    display(metrics_df)

    print('\n')
    fig, ax = plt.subplots(figsize=(12, 6))
    metrics_df.plot(kind='bar', rot=0, ax=ax, width=0.8, cmap='RdBu')

    for i, j in enumerate(ax.patches):
        ax.text(j.get_x() + j.get_width() / 2, j.get_height() + 0.01,
                str(round(j.get_height(), 2)), ha='center')
    plt.xlabel('Metrics')
    plt.ylabel('Scores')
    plt.title('Performance Metrics by Toxicity Type')
    plt.tight_layout()
    plt.show()

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

"""### VII. DNN Model"""

import sys, os, re, csv, codecs, numpy as np, pandas as pd

from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model, Sequential
from keras import initializers, regularizers, constraints, optimizers, layers

model_1 = Sequential([])
model_1.add(layers.Input(shape = (maxlen, )))
model_1.add(layers.Embedding(embedding_matrix.shape[0],
                             embedding_matrix.shape[1],
                             weights = [embedding_matrix]))
model_1.add(layers.Flatten())
model_1.add(layers.Dense(32, activation = 'relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.1))
model_1.add(layers.Dense(16, activation = 'relu'))
model_1.add(layers.BatchNormalization())
model_1.add(layers.Dropout(0.1))
model_1.add(layers.Dense(6, activation = 'sigmoid'))

model_1.summary()

model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])

model_1.fit(tokens_train, y_train, batch_size= 50, epochs= 10, validation_split=0.1);

y_pred = model_1.predict([tokens_dev], batch_size=1024, verbose=1)

y_dev = y_dev.to_numpy()

Model_OverallPerformance(y_pred, y_dev)

"""From the result, we could see that the DNN model does not perform very well on rare subtypes such as severe toxic, threat, and identity hate.

### VIII. CNN Model
"""

model_2 = Sequential([])
model_2.add(layers.Input(shape = (maxlen, )))
model_2.add(layers.Embedding(embedding_matrix.shape[0],
                             embedding_matrix.shape[1],
                             weights = [embedding_matrix]))
model_2.add(layers.Conv1D(128, 5, activation = 'relu'))
model_2.add(layers.GlobalMaxPool1D())
model_2.add(layers.Dense(50, activation = 'relu'))
model_2.add(layers.Dropout(0.1))
model_2.add(layers.Dense(6, activation = 'sigmoid'))

model_2.summary()

model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])

model_2.fit(tokens_train, y_train, batch_size= 50, epochs= 5, validation_split=0.1);

y_pred = model_2.predict([tokens_dev], batch_size=50, verbose=1)

Model_OverallPerformance(y_pred, y_dev)

"""The CNN model performs much better than the DNN model, with significant improvement on rare subtypes.

### IX.CNN + LSTM Model
"""

model_3 = Sequential([])
model_3.add(layers.Input(shape = (maxlen, )))
model_3.add(layers.Embedding(embedding_matrix.shape[0],
                             embedding_matrix.shape[1],
                             weights = [embedding_matrix]))
model_3.add(layers.Conv1D(128, 5, activation = 'relu'))
model_3.add(layers.MaxPool1D())
model_3.add(layers.LSTM(64))
model_3.add(layers.BatchNormalization())
model_3.add(layers.Dense(50, activation = 'relu'))
model_3.add(layers.Dropout(0.1))
model_3.add(layers.Dense(6, activation = 'sigmoid'))

model_3.summary()

model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])

model_3.fit(tokens_train, y_train, batch_size= 50, epochs= 5, validation_split=0.1);

y_pred = model_3.predict([tokens_dev], batch_size=50, verbose=1)

Model_OverallPerformance(y_pred, y_dev)

"""The CNN + LSTM model does not perform better than the CNN model.

### X. RNN BLSTM Models
"""

## 1 layer of BLSTM
model_4 = Sequential([])

model_4.add(layers.Input(shape=(maxlen,)))
model_4.add(layers.Embedding(embedding_matrix.shape[0], 
                           embedding_matrix.shape[1], 
                           weights=[embedding_matrix]))
model_4.add(layers.Bidirectional(layers.LSTM(50,  return_sequences=True)))
model_4.add(layers.GlobalMaxPool1D())
model_4.add(layers.Dense(50, activation='relu'))
model_4.add(layers.Dropout(0.1))
model_4.add(layers.Dense(6, activation="sigmoid"))

model_4.summary()

model_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])

model_4.fit(tokens_train, y_train, batch_size= 50, epochs= 5, validation_split=0.1);

y_pred = model_4.predict([tokens_dev], batch_size=50, verbose=1)

Model_OverallPerformance(y_pred, y_dev)

"""The BLSTM model has the best performance so far."""

# 2 layers of BLSTM
model_5 = Sequential([])

model_5.add(layers.Input(shape=(maxlen,)))
model_5.add(layers.Embedding(embedding_matrix.shape[0], 
                           embedding_matrix.shape[1], 
                           weights=[embedding_matrix]))
model_5.add(layers.Bidirectional(layers.LSTM(50,  return_sequences=True)))
model_5.add(layers.Bidirectional(layers.LSTM(50,  return_sequences=True)))
model_5.add(layers.BatchNormalization())
model_5.add(layers.GlobalMaxPool1D())
model_5.add(layers.Dense(50, activation='relu'))
model_5.add(layers.Dropout(0.1))
model_5.add(layers.Dense(6, activation="sigmoid"))

model_5.summary()

model_5.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])

model_5.fit(tokens_train, y_train, batch_size= 50, epochs= 5, validation_split=0.1);

y_pred = model_5.predict([tokens_dev], batch_size=50, verbose=1)

Model_OverallPerformance(y_pred, y_dev)

"""Two layers of BLSTM do not perform better than one layer of BLSTM. Possibly related to overfitting."""