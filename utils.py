# -*- coding: utf-8 -*-
"""Utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OY_blZPOPvU5iWK2vqAYrHEYiPz83OxY

## Utils

This Python file includes some common functions used throughout the project.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Function 1: Evaluate model performance (basic metrics with plot)"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

def Model_performance(pred, true, plot = True):
    conf_matrix = confusion_matrix(true, pred)
    tn, fp, fn, tp = conf_matrix.ravel()
    print("True Positive (TP): ", tp)
    print("True Negative (TN): ", tn)
    print("False Positive (FP): ", fp)
    print("False Negative (FN): ", fn)
    print("\nMetrics:")
    accuracy = accuracy_score(true, pred)
    print('Accuracy: %.3f' % accuracy)
    precision = precision_score(true, pred)
    print('Precision: %.3f' % precision)
    recall = recall_score(true, pred)
    print('Recall: %.3f' % recall)
    f1 = f1_score(true, pred)
    print('F1 Score: %.3f' % f1)
    print("\n")

    if plot:
        names = ["True Neg","False Pos","False Neg","True Pos"]
        counts = ['{0:0.0f}'.format(value) for value in conf_matrix.flatten()]
        percentages = ['{0:.4%}'.format(value) for value in
                     conf_matrix.flatten()/np.sum(conf_matrix)]
        labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(names,counts,percentages)]
        labels = np.asarray(labels).reshape(2,2)
        sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues')
    
    return accuracy, precision, recall, f1

"""Function 2: Choose the best threshold for subcategory based on F1 score"""

def float_array_to_binary(float_array, threshold):
    binary_array = np.where(np.array(float_array) >=  threshold, 1, 0)
    return binary_array

def chooseThreshold(subtype, true, pred, plot = True):
    max_f1 = 0
    argmax_f1 = 0
    accuracy_list = []
    recall_list = []
    precision_list = []
    f1_list = []
    thres_list = np.arange(0.1, 1.0, 0.1)

    for i in thres_list:
        i = round(i, 2)
        pred_i = float_array_to_binary(pred, i)
        accuracy = accuracy_score(true, pred_i)
        accuracy_list.append(accuracy)
        recall = recall_score(true, pred_i)
        recall_list.append(recall)
        precision = precision_score(true, pred_i)
        precision_list.append(precision)
        f1 = f1_score(true, pred_i)
        f1_list.append(f1)
        if f1 > max_f1:
            max_f1 = f1
            argmax_f1 = i
    
    if plot:
        plt.plot(thres_list, accuracy_list, label ='accuracy')
        plt.plot(thres_list, recall_list, '-.', label ='recall')
        plt.plot(thres_list, precision_list, '-', label ='precision')
        plt.plot(thres_list, f1_list, ':', label ='f1')
        plt.plot(argmax_f1, max_f1,'bo-')
        label = "(" + str(argmax_f1) + ", " + str(round(max_f1,2)) + ")"
        plt.annotate(label, (argmax_f1, max_f1), textcoords="offset points",
                        xytext=(0,10), ha='center') 

        plt.xlabel("threshold")
        plt.ylabel("performance")
        plt.legend()
        plt.title(subtype + ' Subtype : Performance At Different Threshold')
        plt.show()
    return argmax_f1, max_f1

def metrics(pred, true):
    conf_matrix = confusion_matrix(true, pred)
    tn, fp, fn, tp = conf_matrix.ravel()
    accuracy = accuracy_score(true, pred)
    precision = precision_score(true, pred)
    recall = recall_score(true, pred)
    f1 = f1_score(true, pred)
    return round(accuracy,4), round(precision,4), round(recall,4), round(f1,4)

"""Function 3: Evaluate model performance with adjusted threshold for each subcategory"""

def Model_OverallPerformance(y_pred, y_true, adjustThreshold = True, Thresholdplot = False):
    y_true_toxic = [row[0] for row in y_true]
    y_true_severe_toxic = [row[1] for row in y_true]
    y_true_obscene  = [row[2] for row in y_true]
    y_true_threat = [row[3] for row in y_true]
    y_true_insult = [row[4] for row in y_true]
    y_true_identity_threat = [row[5] for row in y_true]

    y_pred_toxic = [row[0] for row in y_pred]
    y_pred_severe_toxic = [row[1] for row in y_pred]
    y_pred_obscene  = [row[2] for row in y_pred]
    y_pred_threat = [row[3] for row in y_pred]
    y_pred_insult = [row[4] for row in y_pred]
    y_pred_identity_threat = [row[5] for row in y_pred]

    if adjustThreshold:
        thresh_toxic, f1_toxic = chooseThreshold('Toxic', y_true_toxic, y_pred_toxic, Thresholdplot)
        thresh_severe_toxic, f1_severe_toxic = chooseThreshold('Severe Toxic', y_true_severe_toxic, y_pred_severe_toxic, Thresholdplot)
        thresh_obscene, f1_obscene = chooseThreshold('Obsence', y_true_obscene, y_pred_obscene, Thresholdplot)
        thresh_threat, f1_threat = chooseThreshold('Threat', y_true_threat, y_pred_threat, Thresholdplot)
        thresh_insult, f1_insult = chooseThreshold('Insult', y_true_insult, y_pred_insult, Thresholdplot)
        thresh_identity_threat, f1_identity_threat = chooseThreshold('Identity Threat', y_true_identity_threat, y_pred_identity_threat, Thresholdplot)

        y_pred_toxic = float_array_to_binary(y_pred_toxic, thresh_toxic)
        y_pred_severe_toxic = float_array_to_binary(y_pred_severe_toxic, thresh_severe_toxic)
        y_pred_obscene = float_array_to_binary(y_pred_obscene, thresh_obscene)
        y_pred_threat = float_array_to_binary(y_pred_threat, thresh_threat)
        y_pred_insult = float_array_to_binary(y_pred_insult, thresh_insult)
        y_pred_identity_threat = float_array_to_binary(y_pred_identity_threat, thresh_identity_threat)

    accuracy_toxic, precision_toxic, recall_toxic, f1_toxic = metrics(y_pred_toxic, y_true_toxic)
    accuracy_severe_toxic, precision_severe_toxic, recall_severe_toxic, f1_severe_toxic = metrics(y_pred_severe_toxic, y_true_severe_toxic)
    accuracy_obscene, precision_obscene, recall_obscene, f1_obscene = metrics(y_pred_obscene, y_true_obscene)
    accuracy_threat, precision_threat, recall_threat, f1_threat = metrics(y_pred_threat, y_true_threat)
    accuracy_insult, precision_insult, recall_insult, f1_insult = metrics(y_pred_insult, y_true_insult)
    accuracy_identity_threat, precision_identity_threat, recall_identity_threat, f1_identity_threat = metrics(y_pred_identity_threat, y_true_identity_threat)


    accuracy_list = [accuracy_toxic, accuracy_severe_toxic, accuracy_obscene, accuracy_threat,accuracy_insult, accuracy_identity_threat]
    precision_list = [precision_toxic, precision_severe_toxic, precision_obscene, precision_threat,precision_insult, precision_identity_threat]
    recall_list = [ recall_toxic,  recall_severe_toxic,  recall_obscene,  recall_threat, recall_insult,  recall_identity_threat]
    f1_list = [f1_toxic, f1_severe_toxic, f1_obscene, f1_threat,f1_insult, f1_identity_threat]

    subtypes = ["Toxic", "Severe_Toxic", "Obscene", "Threat", "Insult", "Identity_Hate"]
    metric = ['Accuracy', 'Precision', 'Recall', 'F1']

    metrics_df = pd.DataFrame(
    data = [accuracy_list, precision_list, recall_list, f1_list],
    columns = subtypes,
    index = metric
    )
    print('\n')

    display(metrics_df)

    print('\n')
    fig, ax = plt.subplots(figsize=(12, 6))
    metrics_df.plot(kind='bar', rot=0, ax=ax, width=0.8, cmap='RdBu')

    for i, j in enumerate(ax.patches):
        ax.text(j.get_x() + j.get_width() / 2, j.get_height() + 0.01,
                str(round(j.get_height(), 2)), ha='center')
    plt.xlabel('Metrics')
    plt.ylabel('Scores')
    plt.title('Performance Metrics by Toxicity Type')
    plt.tight_layout()
    plt.show()

"""Function 4: Plot accuracy and loss of training and validation sets over epoches"""

def plot_training(model_history):
    # Plot the loss function
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))
    ax1.plot(np.sqrt(model_history.history['loss']), 'r', label='train')
    ax1.plot(np.sqrt(model_history.history['val_loss']), 'b' ,label='val')
    ax1.set_xlabel(r'Epoch', fontsize=10)
    ax1.set_ylabel(r'Loss', fontsize=10)
    ax1.legend()
    ax1.set_title('Loss Function Over Epoch')
    ax1.tick_params(labelsize=10)

    # Plot the accuracy
    ax2.plot(np.sqrt(model_history.history['accuracy']), 'r', label='train')
    ax2.plot(np.sqrt(model_history.history['val_accuracy']), 'b' ,label='val')
    ax2.set_xlabel(r'Epoch', fontsize=10)
    ax2.set_ylabel(r'Accuracy', fontsize=10)
    ax2.legend()
    ax2.set_title('Accuracy Over Epoch')
    ax2.tick_params(labelsize=10)